<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>'shape' Dialect - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.64.1"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Dialects/ShapeDialect/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://mlir.llvm.org/js/bundle.js></script><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/master/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/master/mlir>GitHub</a></li></ul></li><li><a href="https://bugs.llvm.org/buglist.cgi?bug_status=__open__&list_id=177877&order=changeddate%20DESC%2Cpriority%2Cbug_severity&product=MLIR&query_format=specific">Bugs</a></li></ul></nav></div><div class=content-container><main><h1>'shape' Dialect</h1><p>Types and operations for shape dialect</p><p>This dialect contains operations for shape inference.</p><p>Note: Unless explicitly stated, all functions that return a shape and take
shapes as input, return the invalid shape if one of its operands is an
invalid shape. This avoids flagging multiple errors for one verification
failure. The dialect itself does not specify how errors should be combined
(there are multiple different options, from always choosing first operand,
concatting etc. on how to combine them).</p><p><nav id=TableOfContents><ul><li><a href=#type-definition>Type definition</a><ul><li><a href=#component-type>component type</a></li><li><a href=#element-type>element type</a></li><li><a href=#shape>shape</a></li><li><a href=#size>size</a></li><li><a href=#value-shape>value shape</a></li><li><a href=#witness>witness</a></li></ul></li><li><a href=#operation-definition>Operation definition</a><ul><li><a href=#shapeadd-shapeaddop>shape.add (shape::AddOp)</a></li><li><a href=#shapeany-shapeanyop>shape.any (shape::AnyOp)</a></li><li><a href=#shapeassuming_all-shapeassumingallop>shape.assuming_all (shape::AssumingAllOp)</a></li><li><a href=#shapeassuming-shapeassumingop>shape.assuming (shape::AssumingOp)</a></li><li><a href=#shapeassuming_yield-shapeassumingyieldop>shape.assuming_yield (shape::AssumingYieldOp)</a></li><li><a href=#shapebroadcast-shapebroadcastop>shape.broadcast (shape::BroadcastOp)</a></li><li><a href=#shapeconcat-shapeconcatop>shape.concat (shape::ConcatOp)</a></li><li><a href=#shapeconst_shape-shapeconstshapeop>shape.const_shape (shape::ConstShapeOp)</a></li><li><a href=#shapeconst_size-shapeconstsizeop>shape.const_size (shape::ConstSizeOp)</a></li><li><a href=#shapeconst_witness-shapeconstwitnessop>shape.const_witness (shape::ConstWitnessOp)</a></li><li><a href=#shapecstr_broadcastable-shapecstrbroadcastableop>shape.cstr_broadcastable (shape::CstrBroadcastableOp)</a></li><li><a href=#shapecstr_eq-shapecstreqop>shape.cstr_eq (shape::CstrEqOp)</a></li><li><a href=#shapedebug_print-shapedebugprintop>shape.debug_print (shape::DebugPrintOp)</a></li><li><a href=#shapefrom_extent_tensor-shapefromextenttensorop>shape.from_extent_tensor (shape::FromExtentTensorOp)</a></li><li><a href=#shapefrom_extents-shapefromextentsop>shape.from_extents (shape::FromExtentsOp)</a></li><li><a href=#shapeget_extent-shapegetextentop>shape.get_extent (shape::GetExtentOp)</a></li><li><a href=#shapeindex_to_size-shapeindextosizeop>shape.index_to_size (shape::IndexToSizeOp)</a></li><li><a href=#shapejoin-shapejoinop>shape.join (shape::JoinOp)</a></li><li><a href=#shapemul-shapemulop>shape.mul (shape::MulOp)</a></li><li><a href=#shapenum_elements-shapenumelementsop>shape.num_elements (shape::NumElementsOp)</a></li><li><a href=#shaperank-shaperankop>shape.rank (shape::RankOp)</a></li><li><a href=#shapereduce-shapereduceop>shape.reduce (shape::ReduceOp)</a></li><li><a href=#shapeshape_eq-shapeshapeeqop>shape.shape_eq (shape::ShapeEqOp)</a></li><li><a href=#shapeshape_of-shapeshapeofop>shape.shape_of (shape::ShapeOfOp)</a></li><li><a href=#shapesize_to_index-shapesizetoindexop>shape.size_to_index (shape::SizeToIndexOp)</a></li><li><a href=#shapesplit_at-shapesplitatop>shape.split_at (shape::SplitAtOp)</a></li><li><a href=#shapeto_extent_tensor-shapetoextenttensorop>shape.to_extent_tensor (shape::ToExtentTensorOp)</a></li><li><a href=#shapewith_shape-shapewithop>shape.with_shape (shape::WithOp)</a></li><li><a href=#shapeyield-shapeyieldop>shape.yield (shape::YieldOp)</a></li></ul></li></ul></nav><h2 id=type-definition>Type definition&nbsp;<a class=headline-hash href=#type-definition>¶</a></h2><h3 id=component-type>component type&nbsp;<a class=headline-hash href=#component-type>¶</a></h3><p><code>shape.element_type</code> represents the element type of the ShapedType. It may
be unknown, error or regular element type supported by ShapedType.</p><h3 id=element-type>element type&nbsp;<a class=headline-hash href=#element-type>¶</a></h3><p><code>shape.element_type</code> represents the element type of the ShapedType. It may
be unknown, error or regular element type supported by ShapedType.</p><h3 id=shape>shape&nbsp;<a class=headline-hash href=#shape>¶</a></h3><p><code>shape.type</code> represents either an unranked shape, a ranked shape with
possibly unknown dimensions or an invalid shape. The rank is of type
<code>shape.size</code> and, if rank is known, the extent is a 1D tensor of type
<code>shape.size</code>.</p><p>Shape is printed:</p><ul><li><code>[*]</code> if it is an unranked shape</li><li><code>[?, 2]</code> if a rank 2 tensor with one unknown dimension</li><li><code>[3, 4]</code> is a rank 2 static tensor</li><li><code>[]</code> is a scalar</li><li><code>[1]</code> is a rank 1 tensor with 1 element</li><li><code>[invalid]</code> for an invalid shape</li></ul><h3 id=size>size&nbsp;<a class=headline-hash href=#size>¶</a></h3><p><code>shape.size</code> represents a non-negative integer with support for being
unknown and invalid.</p><p>Operations on <code>shape.size</code> types are specialized to handle unknown/dynamic
value. So, for example, <code>&lt;unknown> + x == &lt;unknown></code> for all non-error <code>x : !shape.size</code> (e.g., an unknown value does not become known due to addition).</p><h3 id=value-shape>value shape&nbsp;<a class=headline-hash href=#value-shape>¶</a></h3><p><code>shape.value_shape</code> represents the value produced by an operation (this
corresponds to <code>Value</code> in the compiler) and a shape. Conceptually this is a
tuple of a value (potentially unknown) and <code>shape.type</code>. The value and shape
can either or both be unknown. If both the <code>value</code> and <code>shape</code> are known,
then the shape of <code>value</code> is conformant with <code>shape</code>. That is, the shape of
the value conforms to the shape of the ValueShape, so that if we have
ValueShape <code>(value, shape)</code> then <code>join(shape_of(value), shape)</code> would be
error free and in particular it means that if both are statically known,
then they are equal.</p><h3 id=witness>witness&nbsp;<a class=headline-hash href=#witness>¶</a></h3><p>A witness is a structural device in the compiler to maintain ordering of
code relying on information obtained from passing assertions. Witnesses do
not represent any physical data.</p><p>&ldquo;cstr_&rdquo; operations will return witnesses and be lowered into assertion logic
when not resolvable at compile time.</p><p>&ldquo;assuming_&rdquo; operations will take witnesses as input and represent only
information to the compiler, so they do not exist in executing code. Code
that is dependent on &ldquo;assuming_&rdquo; operations can assume all cstr operations
transitively before are honored as true.</p><p>These abstractions are intended to allow the compiler more freedom with
assertions by merely showing the assertion through dataflow at this time
rather than a side effecting operation that acts as a barrier. This can be
viewed similarly to a compiler representation of promises from asynchronous,
possibly crashing assertions. Reliant code will not be reordered to before
the code and non-reliant code can be reordered freely, and there are no
guarantees on the final ordering of the assertions or their related code.</p><h2 id=operation-definition>Operation definition&nbsp;<a class=headline-hash href=#operation-definition>¶</a></h2><h3 id=shapeadd-shapeaddop><code>shape.add</code> (shape::AddOp)&nbsp;<a class=headline-hash href=#shapeadd-shapeaddop>¶</a></h3><p>Addition of sizes and indices</p><p>Syntax:</p><pre><code>operation ::= `shape.add` $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `-&gt;` type($result)
</code></pre><p>Adds two sizes or indices. If either operand is an error it will be
propagated to the result. The operands can be of type <code>size</code> or <code>index</code>. If
at least one of the operands can hold an error, i.e. if it is of type <code>size</code>,
then also the result must be of type <code>size</code>.</p><h4 id=operands>Operands:&nbsp;<a class=headline-hash href=#operands>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>size or index</td></tr><tr><td align=center><code>rhs</code></td><td>size or index</td></tr></tbody></table><h4 id=results>Results:&nbsp;<a class=headline-hash href=#results>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>size or index</td></tr></tbody></table><h3 id=shapeany-shapeanyop><code>shape.any</code> (shape::AnyOp)&nbsp;<a class=headline-hash href=#shapeany-shapeanyop>¶</a></h3><p>Return any combination of the input shapes</p><p>This operation takes multiple input shapes or extent tensors and returns
some combination of their dimensions. This can be best seen with examples
below.</p><p>The result is undefined, but still side-effect free, in cases where the
inputs have differing ranks or differ in extents of shared dimensions.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%s0</span> <span class=p>=</span> shape<span class=p>.</span>any <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=err>?</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=err>?</span><span class=p>,</span><span class=m>3</span><span class=p>]</span> <span class=c>// [2,3]
</span><span class=c></span><span class=nv>%s1</span> <span class=p>=</span> shape<span class=p>.</span>any <span class=p>[</span><span class=err>?</span><span class=p>,</span><span class=err>?</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// [1,2]
</span></code></pre></div><h4 id=operands-1>Operands:&nbsp;<a class=headline-hash href=#operands-1>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>inputs</code></td><td>shape or extent tensor</td></tr></tbody></table><h4 id=results-1>Results:&nbsp;<a class=headline-hash href=#results-1>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape or extent tensor</td></tr></tbody></table><h3 id=shapeassuming_all-shapeassumingallop><code>shape.assuming_all</code> (shape::AssumingAllOp)&nbsp;<a class=headline-hash href=#shapeassuming_all-shapeassumingallop>¶</a></h3><p>Return a logical AND of all witnesses</p><p>Syntax:</p><pre><code>operation ::= `shape.assuming_all` $inputs attr-dict
</code></pre><p>Used to simplify constraints as any single failing precondition is enough
to prevent execution.</p><p>&ldquo;assuming&rdquo; operations represent an execution order restriction to the
compiler, information for dependent code to rely on (by assuming), and
nothing else. They should not exist after a program is fully lowered and
ready to execute.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%w0</span> <span class=p>=</span> shape<span class=p>.</span>cstr_broadcastable <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>3</span><span class=p>,</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Passing
</span><span class=c></span><span class=nv>%w1</span> <span class=p>=</span> shape<span class=p>.</span>cstr_broadcastable <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>3</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Failure
</span><span class=c></span><span class=nv>%w2</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Passing
</span><span class=c></span><span class=nv>%wf</span> <span class=p>=</span> shape<span class=p>.</span>assuming_all <span class=nv>%w0</span><span class=p>,</span> <span class=nv>%w1</span> <span class=c>// Failure
</span><span class=c></span><span class=nv>%wt</span> <span class=p>=</span> shape<span class=p>.</span>assuming_all <span class=nv>%w0</span><span class=p>,</span> <span class=nv>%w2</span> <span class=c>// Passing
</span></code></pre></div><h4 id=operands-2>Operands:&nbsp;<a class=headline-hash href=#operands-2>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>inputs</code></td><td>witness</td></tr></tbody></table><h4 id=results-2>Results:&nbsp;<a class=headline-hash href=#results-2>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>witness</td></tr></tbody></table><h3 id=shapeassuming-shapeassumingop><code>shape.assuming</code> (shape::AssumingOp)&nbsp;<a class=headline-hash href=#shapeassuming-shapeassumingop>¶</a></h3><p>Execute the region</p><p>Executes the region assuming all witnesses are true.</p><p>&ldquo;assuming&rdquo; operations represent an execution order restriction to the
compiler, information for dependent code to rely on (by assuming), and
nothing else. They should not exist after a program is fully lowered and
ready to execute.</p><h4 id=operands-3>Operands:&nbsp;<a class=headline-hash href=#operands-3>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>witness</code></td><td>witness</td></tr></tbody></table><h4 id=results-3>Results:&nbsp;<a class=headline-hash href=#results-3>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>results</code></td><td>any type</td></tr></tbody></table><h3 id=shapeassuming_yield-shapeassumingyieldop><code>shape.assuming_yield</code> (shape::AssumingYieldOp)&nbsp;<a class=headline-hash href=#shapeassuming_yield-shapeassumingyieldop>¶</a></h3><p>Yield operation</p><p>Syntax:</p><pre><code>operation ::= `shape.assuming_yield` attr-dict ($operands^ `:` type($operands))?
</code></pre><p>This yield operation represents a return operation within the assert_and_exec
region. The operation takes variable number of operands and produces no
results. The operand number and types must match the return signature of
the region that contains the operation.</p><h4 id=operands-4>Operands:&nbsp;<a class=headline-hash href=#operands-4>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>operands</code></td><td>any type</td></tr></tbody></table><h3 id=shapebroadcast-shapebroadcastop><code>shape.broadcast</code> (shape::BroadcastOp)&nbsp;<a class=headline-hash href=#shapebroadcast-shapebroadcastop>¶</a></h3><p>Returns the broadcasted output shape of two inputs</p><p>Syntax:</p><pre><code>operation ::= `shape.broadcast` $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `-&gt;` type($result)
</code></pre><p>Returns the broadcasted shape for two input shapes or extent tensors. Both
operands can be of type <code>shape.shape</code> or <code>tensor&lt;?xindex></code>. The result is of
type <code>shape.shape</code> and, if both operands are tensors, may be of type
<code>tensor&lt;?xindex></code>.</p><p>If the two operand shapes are of different rank the smaller one is padded
with 1&rsquo;s from the left. The resulting broadcasted shape is then defined as</p><pre><code>result[i] = lhs[i] if lhs[i] == rhs[i]
          = lhs[i] if rhs[i] == 1
          = rhs[i] if lhs[i] == 1.
</code></pre><p>In case the resulting shape is undefined, i.e. if corresponding extents are
different from each other but none is 1, the result is an error shape.
Likewise error values are propagated if any of the operands holds an error
value. If the result type is an extent tensor (and can therefore not hold
the error value) the behavior may be undefined. The optional string
attribute can be used to describe the error case.</p><h4 id=attributes>Attributes:&nbsp;<a class=headline-hash href=#attributes>¶</a></h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>error</code></td><td align=center>::mlir::StringAttr</td><td>string attribute</td></tr></tbody></table><h4 id=operands-5>Operands:&nbsp;<a class=headline-hash href=#operands-5>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>shape or extent tensor</td></tr><tr><td align=center><code>rhs</code></td><td>shape or extent tensor</td></tr></tbody></table><h4 id=results-4>Results:&nbsp;<a class=headline-hash href=#results-4>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape or extent tensor</td></tr></tbody></table><h3 id=shapeconcat-shapeconcatop><code>shape.concat</code> (shape::ConcatOp)&nbsp;<a class=headline-hash href=#shapeconcat-shapeconcatop>¶</a></h3><p>Concatenates two shapes</p><p>Syntax:</p><pre><code>operation ::= `shape.concat` $lhs `,` $rhs attr-dict
</code></pre><p>Creates a shape whose dimensions consist of first the dimensions from <code>lhs</code>
followed by the dimensions of <code>rhs</code>.</p><p>Example:
concat([2,3], [4,5]) -> [2,3,4,5]
concat([], []) -> []
concat([], [4,5,6]) -> [4,5,6]</p><h4 id=operands-6>Operands:&nbsp;<a class=headline-hash href=#operands-6>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>shape</td></tr><tr><td align=center><code>rhs</code></td><td>shape</td></tr></tbody></table><h4 id=results-5>Results:&nbsp;<a class=headline-hash href=#results-5>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape</td></tr></tbody></table><h3 id=shapeconst_shape-shapeconstshapeop><code>shape.const_shape</code> (shape::ConstShapeOp)&nbsp;<a class=headline-hash href=#shapeconst_shape-shapeconstshapeop>¶</a></h3><p>Creates a constant shape or extent tensor</p><p>Creates a constant shape or extent tensor. The individual extents are given
as the <code>shape</code> attribute. The number of these values equals the shape&rsquo;s
rank.</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%0</span> <span class=p>=</span> shape<span class=p>.</span>const_shape <span class=p>[</span><span class=p>]</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape
<span class=nv>%1</span> <span class=p>=</span> shape<span class=p>.</span>const_shape <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>2</span><span class=p>,</span> <span class=m>3</span><span class=p>]</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape
<span class=nv>%2</span> <span class=p>=</span> shape<span class=p>.</span>const_shape <span class=p>[</span><span class=m>4</span><span class=p>,</span> <span class=m>5</span><span class=p>,</span> <span class=m>6</span><span class=p>]</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-1>Attributes:&nbsp;<a class=headline-hash href=#attributes-1>¶</a></h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>shape</code></td><td align=center>::mlir::DenseIntElementsAttr</td><td>index elements attribute</td></tr></tbody></table><h4 id=results-6>Results:&nbsp;<a class=headline-hash href=#results-6>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape or extent tensor</td></tr></tbody></table><h3 id=shapeconst_size-shapeconstsizeop><code>shape.const_size</code> (shape::ConstSizeOp)&nbsp;<a class=headline-hash href=#shapeconst_size-shapeconstsizeop>¶</a></h3><p>Creates a constant of type <code>shape.size</code></p><p>Syntax:</p><pre><code>operation ::= `shape.const_size` $value attr-dict
</code></pre><p>Creates a <code>shape.size</code> type representing the constant size given by <code>value</code>.</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%x</span> <span class=p>=</span> shape<span class=p>.</span>const_size <span class=m>10</span>
</code></pre></div><h4 id=attributes-2>Attributes:&nbsp;<a class=headline-hash href=#attributes-2>¶</a></h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>value</code></td><td align=center>::mlir::IntegerAttr</td><td>index attribute</td></tr></tbody></table><h4 id=results-7>Results:&nbsp;<a class=headline-hash href=#results-7>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>size</td></tr></tbody></table><h3 id=shapeconst_witness-shapeconstwitnessop><code>shape.const_witness</code> (shape::ConstWitnessOp)&nbsp;<a class=headline-hash href=#shapeconst_witness-shapeconstwitnessop>¶</a></h3><p>An operation that returns a statically known witness value</p><p>Syntax:</p><pre><code>operation ::= `shape.const_witness` $passing attr-dict
</code></pre><p>This operation represents a statically known witness result. This can be
often used to canonicalize/fold constraint and assuming code that will always
pass.</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%0</span> <span class=p>=</span> shape<span class=p>.</span>const_shape <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>,</span><span class=m>3</span><span class=p>]</span>
<span class=nv>%1</span> <span class=p>=</span> shape<span class=p>.</span>const_shape <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>2</span><span class=p>,</span> <span class=m>3</span><span class=p>]</span>
<span class=nv>%w0</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq<span class=p>(</span><span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>)</span> <span class=c>// Can be folded to &#34;const_witness true&#34;
</span><span class=c></span><span class=nv>%w1</span> <span class=p>=</span> shape<span class=p>.</span>const_witness true
<span class=nv>%w2</span> <span class=p>=</span> shape<span class=p>.</span>assuming_all<span class=p>(</span><span class=nv>%w0</span><span class=p>,</span> <span class=nv>%w2</span><span class=p>)</span> <span class=c>// Can be folded to &#34;const_witness true&#34;
</span></code></pre></div><h4 id=attributes-3>Attributes:&nbsp;<a class=headline-hash href=#attributes-3>¶</a></h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>passing</code></td><td align=center>::mlir::BoolAttr</td><td>bool attribute</td></tr></tbody></table><h4 id=results-8>Results:&nbsp;<a class=headline-hash href=#results-8>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>witness</td></tr></tbody></table><h3 id=shapecstr_broadcastable-shapecstrbroadcastableop><code>shape.cstr_broadcastable</code> (shape::CstrBroadcastableOp)&nbsp;<a class=headline-hash href=#shapecstr_broadcastable-shapecstrbroadcastableop>¶</a></h3><p>Determines if 2 shapes can be successfully broadcasted</p><p>Syntax:</p><pre><code>operation ::= `shape.cstr_broadcastable` $lhs `,` $rhs `:` type($lhs) `,` type($rhs) attr-dict
</code></pre><p>Given two input shapes or extent tensors, return a witness specifying if
they are broadcastable. This broadcastable follows the same logic as what
shape.broadcast documents.</p><p>&ldquo;cstr&rdquo; operations represent runtime assertions.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%w0</span> <span class=p>=</span> shape<span class=p>.</span>cstr_broadcastable <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>3</span><span class=p>,</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Passing
</span><span class=c></span><span class=nv>%w1</span> <span class=p>=</span> shape<span class=p>.</span>cstr_broadcastable <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>3</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Failure
</span></code></pre></div><h4 id=operands-7>Operands:&nbsp;<a class=headline-hash href=#operands-7>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>shape or extent tensor</td></tr><tr><td align=center><code>rhs</code></td><td>shape or extent tensor</td></tr></tbody></table><h4 id=results-9>Results:&nbsp;<a class=headline-hash href=#results-9>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>witness</td></tr></tbody></table><h3 id=shapecstr_eq-shapecstreqop><code>shape.cstr_eq</code> (shape::CstrEqOp)&nbsp;<a class=headline-hash href=#shapecstr_eq-shapecstreqop>¶</a></h3><p>Determines if all input shapes are equal</p><p>Syntax:</p><pre><code>operation ::= `shape.cstr_eq` $inputs attr-dict
</code></pre><p>Given 1 or more input shapes, determine if all shapes are the exact same.</p><p>&ldquo;cstr&rdquo; operations represent runtime assertions.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%w0</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Passing
</span><span class=c></span><span class=nv>%w1</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Failure
</span></code></pre></div><h4 id=operands-8>Operands:&nbsp;<a class=headline-hash href=#operands-8>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>inputs</code></td><td>shape</td></tr></tbody></table><h4 id=results-10>Results:&nbsp;<a class=headline-hash href=#results-10>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>witness</td></tr></tbody></table><h3 id=shapedebug_print-shapedebugprintop><code>shape.debug_print</code> (shape::DebugPrintOp)&nbsp;<a class=headline-hash href=#shapedebug_print-shapedebugprintop>¶</a></h3><p>Prints the input shape or size</p><p>Prints the input dim or shape and passes through input.</p><p>Note: This is intended for testing and debugging only.</p><h4 id=operands-9>Operands:&nbsp;<a class=headline-hash href=#operands-9>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>input</code></td><td>shape or size</td></tr></tbody></table><h4 id=results-11>Results:&nbsp;<a class=headline-hash href=#results-11>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>output</code></td><td>shape or size</td></tr></tbody></table><h3 id=shapefrom_extent_tensor-shapefromextenttensorop><code>shape.from_extent_tensor</code> (shape::FromExtentTensorOp)&nbsp;<a class=headline-hash href=#shapefrom_extent_tensor-shapefromextenttensorop>¶</a></h3><p>Creates a shape from a tensor of extents</p><p>Syntax:</p><pre><code>operation ::= `shape.from_extent_tensor` attr-dict $input `:` type($input)
</code></pre><p>Creates a shape from a 1D integral tensor of extents. The rank of the
resulting shape equals the number of elements in the tensor, and the
extents match the values of the elements.</p><h4 id=operands-10>Operands:&nbsp;<a class=headline-hash href=#operands-10>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>input</code></td><td>tensor of index values</td></tr></tbody></table><h4 id=results-12>Results:&nbsp;<a class=headline-hash href=#results-12>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape</td></tr></tbody></table><h3 id=shapefrom_extents-shapefromextentsop><code>shape.from_extents</code> (shape::FromExtentsOp)&nbsp;<a class=headline-hash href=#shapefrom_extents-shapefromextentsop>¶</a></h3><p>Creates a shape from extents</p><p>Syntax:</p><pre><code>operation ::= `shape.from_extents` $extents attr-dict
</code></pre><p>Creates a shape from multiple SSA values representing the extents of
the shape.</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=c>// Rank 2 shape.
</span><span class=c></span><span class=nv>%s0</span> <span class=p>=</span> shape<span class=p>.</span>from_extents <span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span>
<span class=c>// Rank 0 shape.
</span><span class=c></span><span class=nv>%s1</span> <span class=p>=</span> shape<span class=p>.</span>from_extents
</code></pre></div><h4 id=operands-11>Operands:&nbsp;<a class=headline-hash href=#operands-11>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>extents</code></td><td>index</td></tr></tbody></table><h4 id=results-13>Results:&nbsp;<a class=headline-hash href=#results-13>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>shape</code></td><td>shape</td></tr></tbody></table><h3 id=shapeget_extent-shapegetextentop><code>shape.get_extent</code> (shape::GetExtentOp)&nbsp;<a class=headline-hash href=#shapeget_extent-shapegetextentop>¶</a></h3><p>Gets the specified extent from a shape or extent tensor</p><p>Syntax:</p><pre><code>operation ::= `shape.get_extent` $shape `,` $dim `:` type($shape) `,` type($dim) `-&gt;` type($extent) attr-dict
</code></pre><p>Gets the extent indexed by <code>dim</code> from the <code>shape</code> operand. If the shape is
an error then it returns an error size.</p><h4 id=operands-12>Operands:&nbsp;<a class=headline-hash href=#operands-12>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>shape</code></td><td>shape or extent tensor</td></tr><tr><td align=center><code>dim</code></td><td>size or index</td></tr></tbody></table><h4 id=results-14>Results:&nbsp;<a class=headline-hash href=#results-14>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>extent</code></td><td>size or index</td></tr></tbody></table><h3 id=shapeindex_to_size-shapeindextosizeop><code>shape.index_to_size</code> (shape::IndexToSizeOp)&nbsp;<a class=headline-hash href=#shapeindex_to_size-shapeindextosizeop>¶</a></h3><p>Converts a standard index to a shape size</p><p>Syntax:</p><pre><code>operation ::= `shape.index_to_size` $arg attr-dict
</code></pre><p>Converts a standard index to a <code>shape.size</code>. This operation and its
inverse, <code>size_to_index</code>, facilitate index conversion between the standard
and the shape dialect.</p><p>The behavior is undefined for negative indices.</p><h4 id=operands-13>Operands:&nbsp;<a class=headline-hash href=#operands-13>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>arg</code></td><td>index</td></tr></tbody></table><h4 id=results-15>Results:&nbsp;<a class=headline-hash href=#results-15>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>size</td></tr></tbody></table><h3 id=shapejoin-shapejoinop><code>shape.join</code> (shape::JoinOp)&nbsp;<a class=headline-hash href=#shapejoin-shapejoinop>¶</a></h3><p>Returns the least general shape.size of its operands</p><p>An operation that computes the least general shape of input operands.
This effectively asserts that corresponding static dimensions are equal.
The behavior is to match each element of the <code>shape.shape</code> and propagate the
most restrictive information, returning an invalid shape if there are
contradictory requirements. E.g., using pseudo code</p><pre><code>shape.join([*], [*]) -&gt; [*]
shape.join([*], [1, ?]) -&gt; [1, ?]
shape.join([1, 2], [1, ?]) -&gt; [1, 2]
shape.join([*], [1, 2]) -&gt; [1, 2]
shape.join([], []) -&gt; []
shape.join([], [*]) -&gt; []
shape.join([], [?, ?]) -&gt; [invalid]
shape.join([1, ?], [2, ?, ?]) -&gt; [invalid]
</code></pre><p><code>shape.join</code> also allows specifying an optional error string, that may be
used to return an error to the user upon mismatch of dimensions.</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%c</span> <span class=p>=</span> shape<span class=p>.</span>join <span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span><span class=p>,</span> <span class=nl>error=</span><span class=s>&#34;&lt;reason&gt;&#34;</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape
</code></pre></div><h4 id=attributes-4>Attributes:&nbsp;<a class=headline-hash href=#attributes-4>¶</a></h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>error</code></td><td align=center>::mlir::StringAttr</td><td>string attribute</td></tr></tbody></table><h4 id=operands-14>Operands:&nbsp;<a class=headline-hash href=#operands-14>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>arg0</code></td><td>shape or size</td></tr><tr><td align=center><code>arg1</code></td><td>shape or size</td></tr></tbody></table><h4 id=results-16>Results:&nbsp;<a class=headline-hash href=#results-16>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape or size</td></tr></tbody></table><h3 id=shapemul-shapemulop><code>shape.mul</code> (shape::MulOp)&nbsp;<a class=headline-hash href=#shapemul-shapemulop>¶</a></h3><p>Multiplication of sizes and indices</p><p>Syntax:</p><pre><code>operation ::= `shape.mul` $lhs `,` $rhs `:` type($lhs) `,` type($rhs) `-&gt;` type($result) attr-dict
</code></pre><p>Multiplies two sizes or indices. If either operand is an error it will be
propagated to the result. The operands can be of type <code>size</code> or <code>index</code>. If
at least one of the operands can hold an error, i.e. if it is of type <code>size</code>,
then also the result must be of type <code>size</code>. If error propagation is not
possible because both operands are of type <code>index</code> then the result must also
be of type <code>index</code>.</p><h4 id=operands-15>Operands:&nbsp;<a class=headline-hash href=#operands-15>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>size or index</td></tr><tr><td align=center><code>rhs</code></td><td>size or index</td></tr></tbody></table><h4 id=results-17>Results:&nbsp;<a class=headline-hash href=#results-17>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>size or index</td></tr></tbody></table><h3 id=shapenum_elements-shapenumelementsop><code>shape.num_elements</code> (shape::NumElementsOp)&nbsp;<a class=headline-hash href=#shapenum_elements-shapenumelementsop>¶</a></h3><p>Returns the number of elements for a given shape</p><p>Syntax:</p><pre><code>operation ::= `shape.num_elements` $shape `:` type($shape) `-&gt;` type($result) attr-dict
</code></pre><p>Returns the number of elements for a given shape which is the product of its
extents. If the argument is of type <code>shape</code> then the result will be of type
<code>size</code> and potential errors will be propagated. Otherwise, if the argument
is and extent tensor <code>tensor&lt;?xindex></code> then the result will be of type
<code>index</code>.</p><h4 id=operands-16>Operands:&nbsp;<a class=headline-hash href=#operands-16>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>shape</code></td><td>shape or extent tensor</td></tr></tbody></table><h4 id=results-18>Results:&nbsp;<a class=headline-hash href=#results-18>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>size or index</td></tr></tbody></table><h3 id=shaperank-shaperankop><code>shape.rank</code> (shape::RankOp)&nbsp;<a class=headline-hash href=#shaperank-shaperankop>¶</a></h3><p>Gets the rank of a shape</p><p>Syntax:</p><pre><code>operation ::= `shape.rank` $shape `:` type($shape) `-&gt;` type($rank) attr-dict
</code></pre><p>Returns the rank of the shape or extent tensor, i.e. the number of extents.</p><h4 id=operands-17>Operands:&nbsp;<a class=headline-hash href=#operands-17>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>shape</code></td><td>shape or extent tensor</td></tr></tbody></table><h4 id=results-19>Results:&nbsp;<a class=headline-hash href=#results-19>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>rank</code></td><td>size or index</td></tr></tbody></table><h3 id=shapereduce-shapereduceop><code>shape.reduce</code> (shape::ReduceOp)&nbsp;<a class=headline-hash href=#shapereduce-shapereduceop>¶</a></h3><p>Returns an expression reduced over a shape or extent tensor</p><p>An operation that takes as input a shape or extent tensor, and a number of
initial values. This operation has a region/function that is applied
repeatedly for every extent of the input. Starting with the initial values,
the individual extents are then aggregated as defined by the associated
region.</p><p>Conceptually this op performs the following reduction:</p><pre><code>res[] = init;
for (int i = 0, i &lt; shape.rank(); i++) {
  res = fn(i, shape[i], res[0], ..., res[n]);
}
</code></pre><p>Where <code>fn</code> is provided by the user and the result of the reduce op is the
last computed output of the reduce function. As an example, computing the
number of elements can be defined as follows:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=kt>func</span> <span class=nf>@reduce</span><span class=p>(</span><span class=nv>%shape</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape<span class=p>,</span> <span class=nv>%init</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size <span class=p>{</span>
  <span class=nv>%num_elements</span> <span class=p>=</span> shape<span class=p>.</span>reduce<span class=p>(</span><span class=nv>%shape</span><span class=p>,</span> <span class=nv>%init</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size  <span class=p>{</span>
    <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%index</span><span class=p>:</span> <span class=k>index</span><span class=p>,</span> <span class=nv>%dim</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>,</span> <span class=nv>%acc</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span><span class=p>:</span>
      <span class=nv>%updated_acc</span> <span class=p>=</span> <span class=s>&#34;shape.mul&#34;</span><span class=p>(</span><span class=nv>%acc</span><span class=p>,</span> <span class=nv>%dim</span><span class=p>)</span> <span class=p>:</span>
        <span class=p>(</span><span class=p>!</span>shape<span class=p>.</span>size<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size
      shape<span class=p>.</span>yield <span class=nv>%updated_acc</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size
  <span class=p>}</span>
  <span class=kt>return</span> <span class=nv>%num_elements</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size
<span class=p>}</span>
</code></pre></div><h4 id=operands-18>Operands:&nbsp;<a class=headline-hash href=#operands-18>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>shape</code></td><td>shape or extent tensor</td></tr><tr><td align=center><code>initVals</code></td><td>any type</td></tr></tbody></table><h4 id=results-20>Results:&nbsp;<a class=headline-hash href=#results-20>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>any type</td></tr></tbody></table><h3 id=shapeshape_eq-shapeshapeeqop><code>shape.shape_eq</code> (shape::ShapeEqOp)&nbsp;<a class=headline-hash href=#shapeshape_eq-shapeshapeeqop>¶</a></h3><p>Returns whether the input shapes or extent tensors are equal</p><p>Syntax:</p><pre><code>operation ::= `shape.shape_eq` $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs)
</code></pre><p>Takes two shape or extent tensor operands and determines whether they are
equal. When extent tensors are compared to shapes they are regarded as their
equivalent non-error shapes. Error shapes can be tested for equality like
any other shape value, meaning that the error value is equal to itself.</p><h4 id=operands-19>Operands:&nbsp;<a class=headline-hash href=#operands-19>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>shape or extent tensor</td></tr><tr><td align=center><code>rhs</code></td><td>shape or extent tensor</td></tr></tbody></table><h4 id=results-21>Results:&nbsp;<a class=headline-hash href=#results-21>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>1-bit signless integer</td></tr></tbody></table><h3 id=shapeshape_of-shapeshapeofop><code>shape.shape_of</code> (shape::ShapeOfOp)&nbsp;<a class=headline-hash href=#shapeshape_of-shapeshapeofop>¶</a></h3><p>Returns shape of a value or shaped type operand</p><p>Syntax:</p><pre><code>operation ::= `shape.shape_of` $arg `:` type($arg) `-&gt;` type($result) attr-dict
</code></pre><p>The operation takes a value or a shaped operand as an argument and it
returns a shape or extent tensor.</p><h4 id=operands-20>Operands:&nbsp;<a class=headline-hash href=#operands-20>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>arg</code></td><td>shaped of any type values or value shape</td></tr></tbody></table><h4 id=results-22>Results:&nbsp;<a class=headline-hash href=#results-22>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape or extent tensor</td></tr></tbody></table><h3 id=shapesize_to_index-shapesizetoindexop><code>shape.size_to_index</code> (shape::SizeToIndexOp)&nbsp;<a class=headline-hash href=#shapesize_to_index-shapesizetoindexop>¶</a></h3><p>Casts between index types of the shape and standard dialect</p><p>Syntax:</p><pre><code>operation ::= `shape.size_to_index` $arg attr-dict `:` type($arg)
</code></pre><p>Converts a <code>shape.size</code> to a standard index. This operation and its
inverse, <code>index_to_size</code>, facilitate index conversion between the standard
and the shape dialect. The behavior is undefined for unknown and invalid
arguments.</p><h4 id=operands-21>Operands:&nbsp;<a class=headline-hash href=#operands-21>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>arg</code></td><td>size or index</td></tr></tbody></table><h4 id=results-23>Results:&nbsp;<a class=headline-hash href=#results-23>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>index</td></tr></tbody></table><h3 id=shapesplit_at-shapesplitatop><code>shape.split_at</code> (shape::SplitAtOp)&nbsp;<a class=headline-hash href=#shapesplit_at-shapesplitatop>¶</a></h3><p>Splits a shape at a given index</p><p>Splits a shape at a given dimension <code>index</code>, returning two shapes.
If <code>index</code> is negative, it is treated as indexing from the back of the
shape. This negative-handling behavior is important when handling unranked
shapes, where the positive index is not necessarily knowable due to a
dynamic number of leading dimensions.</p><p>Examples:</p><ul><li>split_at([4,5,6], index=0) -> [], [4,5,6]</li><li>split_at([4,5,6], index=1) -> [4], [5,6]</li><li>split_at([4,5,6], index=2) -> [4,5], [6]</li><li>split_at([4,5,6], index=3) -> [4,5,6], []</li><li>split_at([4,5,6], index=4) -> error</li><li>split_at([4,5,6], index=-1) -> [4,5], [6]</li><li>split_at([4,5,6], index=-2) -> [4], [5,6]</li><li>split_at([4,5,6], index=-3) -> [], [4,5,6]</li><li>split_at([4,5,6], index=-4) -> error</li></ul><p>Requires:</p><ul><li><code>index</code> is in the range [-rank(operand),rank(operand)]</li></ul><h4 id=operands-22>Operands:&nbsp;<a class=headline-hash href=#operands-22>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>operand</code></td><td>shape or extent tensor</td></tr><tr><td align=center><code>index</code></td><td>32-bit signless integer</td></tr></tbody></table><h4 id=results-24>Results:&nbsp;<a class=headline-hash href=#results-24>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>head</code></td><td>shape</td></tr><tr><td align=center><code>tail</code></td><td>shape</td></tr></tbody></table><h3 id=shapeto_extent_tensor-shapetoextenttensorop><code>shape.to_extent_tensor</code> (shape::ToExtentTensorOp)&nbsp;<a class=headline-hash href=#shapeto_extent_tensor-shapetoextenttensorop>¶</a></h3><p>Creates a dimension tensor from a shape</p><p>Syntax:</p><pre><code>operation ::= `shape.to_extent_tensor` attr-dict $input `:` type($input) `-&gt;` type($result)
</code></pre><p>Converts a shape to a 1D integral tensor of extents. The number of elements
in the tensor equals the rank of the shape, and the elements equal the
extents of the shape.</p><p>If the shape represents an error, this op&rsquo;s behavior is undefined.</p><h4 id=operands-23>Operands:&nbsp;<a class=headline-hash href=#operands-23>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>input</code></td><td>shape or extent tensor</td></tr></tbody></table><h4 id=results-25>Results:&nbsp;<a class=headline-hash href=#results-25>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>tensor of index values</td></tr></tbody></table><h3 id=shapewith_shape-shapewithop><code>shape.with_shape</code> (shape::WithOp)&nbsp;<a class=headline-hash href=#shapewith_shape-shapewithop>¶</a></h3><p>Returns ValueShape with given shape</p><p>Syntax:</p><pre><code>operation ::= `shape.with_shape` operands attr-dict `:` type($operand) `,` type($shape)
</code></pre><p>Returns ValueShape with the shape updated to match the shape operand. That
is a new ValueShape tuple is created with value equal to <code>operand</code>'s
value and shape equal to <code>shape</code>. If the ValueShape and given <code>shape</code> are
non-conformant, then the returned ValueShape will represent an error of
this mismatch. Similarly if either inputs are in an error state, then an
error is popagated.</p><p>Usage:
%0 = shape.with_shape %1, %2 : tensor&lt;&mldr;>, !shape.shape</p><p>This is used, for example, where one combines shape function calculations
and/or call one shape function from another. E.g.,</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=kt>func</span> <span class=nf>@shape_foobah</span><span class=p>(</span><span class=nv>%a</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>,</span>
                   <span class=nv>%b</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>,</span>
                   <span class=nv>%c</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape <span class=p>{</span>
  <span class=nv>%0</span> <span class=p>=</span> call <span class=nf>@shape_foo</span><span class=p>(</span><span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span><span class=p>)</span> <span class=p>:</span>
    <span class=p>(</span><span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
  <span class=nv>%1</span> <span class=p>=</span> shape<span class=p>.</span>with_shape <span class=nv>%b</span><span class=p>,</span> <span class=nv>%0</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>shape
  <span class=nv>%2</span> <span class=p>=</span> call <span class=nf>@shape_bah</span><span class=p>(</span><span class=nv>%c</span><span class=p>,</span> <span class=nv>%1</span><span class=p>)</span> <span class=p>:</span>
    <span class=p>(</span><span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
  <span class=kt>return</span> <span class=nv>%2</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape
<span class=p>}</span>
</code></pre></div><p>This op need not be a refinement of the shape. In non-error cases the input
ValueShape&rsquo;s value and shape are conformant and so too for the output, but
the result may be less specified than <code>operand</code>'s shape as <code>shape</code> is
merely used to construct the new ValueShape. If join behavior is desired
then a join op should be used.</p><h4 id=operands-24>Operands:&nbsp;<a class=headline-hash href=#operands-24>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>operand</code></td><td>shaped of any type values or value shape</td></tr><tr><td align=center><code>shape</code></td><td>shape</td></tr></tbody></table><h4 id=results-26>Results:&nbsp;<a class=headline-hash href=#results-26>¶</a></h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>value shape</td></tr></tbody></table><h3 id=shapeyield-shapeyieldop><code>shape.yield</code> (shape::YieldOp)&nbsp;<a class=headline-hash href=#shapeyield-shapeyieldop>¶</a></h3><p>Returns the value to parent op</p><p>Syntax:</p><pre><code>operation ::= `shape.yield` attr-dict ($operands^ `:` type($operands))?
</code></pre><h4 id=operands-25>Operands:&nbsp;<a class=headline-hash href=#operands-25>¶</a></h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>operands</code></td><td>any type</td></tr></tbody></table><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=/docs/Dialects/SCFDialect/ title="'scf' Dialect"><i class="fas fa-arrow-left" aria-hidden=true></i>Prev - 'scf' Dialect</a>
<a class="nav nav-next" href=/docs/Dialects/SPIR-V/ title="'spv' Dialect">Next - 'spv' Dialect <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=/talks/>Talks and Related Publications</a></li><li><a href=/users/>Users of MLIR</a></li><li class=has-sub-menu><a href=/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/getting_started/Debugging/>Debugging</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li><a href=/getting_started/Contributing/>How to Contribute</a></li><li><a href=/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=/getting_started/openprojects/>Open Projects</a></li><li><a href=/getting_started/Glossary/>Glossary</a></li><li><a href=/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=/docs/Bindings/>Bindings<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Bindings/Python/>MLIR Python Bindings</a></li></ul></li><li class="parent has-sub-menu"><a href=/docs/Dialects/>Dialects<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=/docs/Dialects/Linalg/>'linalg' Dialect</a></li><li><a href=/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li><a href=/docs/Dialects/OpenMPDialect/>'omp' Dialect</a></li><li><a href=/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=/docs/Dialects/SCFDialect/>'scf' Dialect</a></li><li class=active><a href=/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=/docs/Dialects/SPIR-V/>'spv' Dialect</a></li><li><a href=/docs/Dialects/Standard/>'std' Dialect</a></li><li><a href=/docs/Dialects/Vector/>'vector' Dialect</a></li></ul></li><li class=has-sub-menu><a href=/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=/docs/Rationale/UsageOfConst/>Usage of 'Const' in MLIR, for core IR types</a></li></ul></li><li class=has-sub-menu><a href=/docs/Tutorials/>Tutorials<span class="mark closed">+</span></a><ul class=sub-menu><li class=has-sub-menu><a href=/docs/Tutorials/Toy/>Toy<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Tutorial Introduction</a></li><li><a href=/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li><a href=/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=/docs/Tutorials/DefiningAttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li></ul></li><li><a href=/docs/ReducerPasses/></a></li><li><a href=/docs/SCFPasses/></a></li><li><a href=/docs/EDSC/>Background: declarative builders API</a></li><li><a href=/docs/ConversionToLLVMDialect/>Conversion to the LLVM Dialect</a></li><li><a href=/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=/docs/DialectConversion/>Dialect Conversion</a></li><li><a href=/docs/GenericDAGRewriter/>Generic DAG Rewriter Infrastructure</a></li><li><a href=/docs/Interfaces/>Interfaces</a></li><li><a href=/docs/CAPI/>MLIR C API</a></li><li><a href=/docs/LangRef/>MLIR Language Reference</a></li><li><a href=/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=/docs/OpDefinitions/>Operation Definition Specification (ODS)</a></li><li><a href=/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=/docs/Passes/>Passes</a></li><li><a href=/docs/Quantization/>Quantization</a></li><li><a href=/docs/ShapeInference/>Shape Inference</a></li><li><a href=/docs/SPIRVToLLVMDialectConversion/>SPIR-V Dialect to LLVM Dialect conversion manual</a></li><li><a href=/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li><a href=/docs/Traits/>Traits</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i><i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>